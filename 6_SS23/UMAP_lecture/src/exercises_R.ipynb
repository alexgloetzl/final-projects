{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning about the theory of UMAP in the lecture, we want to program parts of the algorithm for ourselves. We will start by calculating the $k$-nearest neighbors of each sample. For this we will look at the iris data set again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa\\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa\\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa\\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa\\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa\\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 5\n",
       "\n",
       "| <!--/--> | Sepal.Length &lt;dbl&gt; | Sepal.Width &lt;dbl&gt; | Petal.Length &lt;dbl&gt; | Petal.Width &lt;dbl&gt; | Species &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 5.1 | 3.5 | 1.4 | 0.2 | setosa |\n",
       "| 2 | 4.9 | 3.0 | 1.4 | 0.2 | setosa |\n",
       "| 3 | 4.7 | 3.2 | 1.3 | 0.2 | setosa |\n",
       "| 4 | 4.6 | 3.1 | 1.5 | 0.2 | setosa |\n",
       "| 5 | 5.0 | 3.6 | 1.4 | 0.2 | setosa |\n",
       "| 6 | 5.4 | 3.9 | 1.7 | 0.4 | setosa |\n",
       "\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1 5.1          3.5         1.4          0.2         setosa \n",
       "2 4.9          3.0         1.4          0.2         setosa \n",
       "3 4.7          3.2         1.3          0.2         setosa \n",
       "4 4.6          3.1         1.5          0.2         setosa \n",
       "5 5.0          3.6         1.4          0.2         setosa \n",
       "6 5.4          3.9         1.7          0.4         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(\"iris\")\n",
    "\n",
    "head(iris)\n",
    "\n",
    "iris_data <- iris[, -5]\n",
    "iris_species <- iris[, 5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general theme in data science is to scale your data beforehand because usually algorithms work best if the features of the data provided to them are on the same order of magnitude. Therefore we will apply the standard scaling, also called z-score, for the iris data set.\n",
    "\n",
    "The z-score is calculated like this:\n",
    "\n",
    "$\\Large \\text{z-score} = \\frac{x-x_{mean}}{x_{SD}}$\n",
    "\n",
    "with $x_{mean}$ and $x_{SD}$ being the mean and the standard deviation of the data, respectively. Instead of calculating the z-score manually, we can use an in-built function `scale(x)` of R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 6 × 4 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>-0.8976739</td><td> 1.01560199</td><td>-1.335752</td><td>-1.311052</td></tr>\n",
       "\t<tr><td>-1.1392005</td><td>-0.13153881</td><td>-1.335752</td><td>-1.311052</td></tr>\n",
       "\t<tr><td>-1.3807271</td><td> 0.32731751</td><td>-1.392399</td><td>-1.311052</td></tr>\n",
       "\t<tr><td>-1.5014904</td><td> 0.09788935</td><td>-1.279104</td><td>-1.311052</td></tr>\n",
       "\t<tr><td>-1.0184372</td><td> 1.24503015</td><td>-1.335752</td><td>-1.311052</td></tr>\n",
       "\t<tr><td>-0.5353840</td><td> 1.93331463</td><td>-1.165809</td><td>-1.048667</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6 × 4 of type dbl\n",
       "\\begin{tabular}{llll}\n",
       " Sepal.Length & Sepal.Width & Petal.Length & Petal.Width\\\\\n",
       "\\hline\n",
       "\t -0.8976739 &  1.01560199 & -1.335752 & -1.311052\\\\\n",
       "\t -1.1392005 & -0.13153881 & -1.335752 & -1.311052\\\\\n",
       "\t -1.3807271 &  0.32731751 & -1.392399 & -1.311052\\\\\n",
       "\t -1.5014904 &  0.09788935 & -1.279104 & -1.311052\\\\\n",
       "\t -1.0184372 &  1.24503015 & -1.335752 & -1.311052\\\\\n",
       "\t -0.5353840 &  1.93331463 & -1.165809 & -1.048667\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6 × 4 of type dbl\n",
       "\n",
       "| Sepal.Length | Sepal.Width | Petal.Length | Petal.Width |\n",
       "|---|---|---|---|\n",
       "| -0.8976739 |  1.01560199 | -1.335752 | -1.311052 |\n",
       "| -1.1392005 | -0.13153881 | -1.335752 | -1.311052 |\n",
       "| -1.3807271 |  0.32731751 | -1.392399 | -1.311052 |\n",
       "| -1.5014904 |  0.09788935 | -1.279104 | -1.311052 |\n",
       "| -1.0184372 |  1.24503015 | -1.335752 | -1.311052 |\n",
       "| -0.5353840 |  1.93331463 | -1.165809 | -1.048667 |\n",
       "\n"
      ],
      "text/plain": [
       "     Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "[1,] -0.8976739    1.01560199 -1.335752    -1.311052  \n",
       "[2,] -1.1392005   -0.13153881 -1.335752    -1.311052  \n",
       "[3,] -1.3807271    0.32731751 -1.392399    -1.311052  \n",
       "[4,] -1.5014904    0.09788935 -1.279104    -1.311052  \n",
       "[5,] -1.0184372    1.24503015 -1.335752    -1.311052  \n",
       "[6,] -0.5353840    1.93331463 -1.165809    -1.048667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris_data <- scale(iris_data)\n",
    "head(iris_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to calculate the k-nearest neighbors of each of our samples. As a distance measure we use the euclidean distance as in the lecture. You can use for-loops or the built-in function `dist()`. Look up how to use `dist()` on the internet. Make sure that the end result is a matrix. You can check the format/class of an object with `str()`.\n",
    "\n",
    "The upper left corner (5x5) of the distance matrix (150x150) should look something like this:\n",
    "\n",
    "| |\t1|\t2|\t3|\t4|\t5|\n",
    "|--- |--- |---|---|---|---|\n",
    "|1\t|0.0000000\t|1.1722914\t|0.8427840\t|1.0999999\t|0.2592702|\n",
    "|2\t|1.1722914\t|0.0000000\t|0.5216255\t|0.4325508\t|1.3818560|\n",
    "|3\t|0.8427840\t|0.5216255\t|0.0000000\t|0.2829432\t|0.9882608|\n",
    "|4\t|1.0999999\t|0.4325508\t|0.2829432\t|0.0000000\t|1.2459861|\n",
    "|5\t|0.2592702\t|1.3818560\t|0.9882608\t|1.2459861\t|0.0000000|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "### Try to code the distance matrix for the iris data set `iris_data`\n",
    "\n",
    "calculate_distance <- function(data){\n",
    "    #' @param data the iris data set 'iris_data'\n",
    "    #' @returns a distance matrix, where first row shows the distance of the first sample to all the other samples. \n",
    "    #' second row shows the distance of the second sample to all the other samples. And so on.\n",
    "    \n",
    "    # here goes your code\n",
    "    \n",
    "    return(distance_matrix)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 5 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0.0000000</td><td>1.1722914</td><td>0.8427840</td><td>1.0999999</td><td>0.2592702</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1.1722914</td><td>0.0000000</td><td>0.5216255</td><td>0.4325508</td><td>1.3818560</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.8427840</td><td>0.5216255</td><td>0.0000000</td><td>0.2829432</td><td>0.9882608</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1.0999999</td><td>0.4325508</td><td>0.2829432</td><td>0.0000000</td><td>1.2459861</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.2592702</td><td>1.3818560</td><td>0.9882608</td><td>1.2459861</td><td>0.0000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 5 of type dbl\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & 1 & 2 & 3 & 4 & 5\\\\\n",
       "\\hline\n",
       "\t1 & 0.0000000 & 1.1722914 & 0.8427840 & 1.0999999 & 0.2592702\\\\\n",
       "\t2 & 1.1722914 & 0.0000000 & 0.5216255 & 0.4325508 & 1.3818560\\\\\n",
       "\t3 & 0.8427840 & 0.5216255 & 0.0000000 & 0.2829432 & 0.9882608\\\\\n",
       "\t4 & 1.0999999 & 0.4325508 & 0.2829432 & 0.0000000 & 1.2459861\\\\\n",
       "\t5 & 0.2592702 & 1.3818560 & 0.9882608 & 1.2459861 & 0.0000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 5 of type dbl\n",
       "\n",
       "| <!--/--> | 1 | 2 | 3 | 4 | 5 |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 0.0000000 | 1.1722914 | 0.8427840 | 1.0999999 | 0.2592702 |\n",
       "| 2 | 1.1722914 | 0.0000000 | 0.5216255 | 0.4325508 | 1.3818560 |\n",
       "| 3 | 0.8427840 | 0.5216255 | 0.0000000 | 0.2829432 | 0.9882608 |\n",
       "| 4 | 1.0999999 | 0.4325508 | 0.2829432 | 0.0000000 | 1.2459861 |\n",
       "| 5 | 0.2592702 | 1.3818560 | 0.9882608 | 1.2459861 | 0.0000000 |\n",
       "\n"
      ],
      "text/plain": [
       "  1         2         3         4         5        \n",
       "1 0.0000000 1.1722914 0.8427840 1.0999999 0.2592702\n",
       "2 1.1722914 0.0000000 0.5216255 0.4325508 1.3818560\n",
       "3 0.8427840 0.5216255 0.0000000 0.2829432 0.9882608\n",
       "4 1.0999999 0.4325508 0.2829432 0.0000000 1.2459861\n",
       "5 0.2592702 1.3818560 0.9882608 1.2459861 0.0000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_distance <- function(iris_data){\n",
    "    #' @param data the iris data set 'iris_data'\n",
    "    #' @returns a distance matrix\n",
    "    \n",
    "    dist <- as.matrix(dist(iris_data, method = \"euclidean\", diag = TRUE, upper = TRUE))\n",
    "    \n",
    "    return(dist)\n",
    "}\n",
    "\n",
    "dist <- calculate_distance(iris_data)\n",
    "dist[1:5,1:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the distance to the next nearest neighbor $d_{\\text{nearest neighbor}}$ for each sample. You can use the distance matrix from above and the `sort()` or `order()` function. The end result should be an array/vector of length 150 since we have that many samples.\n",
    "\n",
    "The distance to the 5 next nearest neighbors of the first five samples should look something like this:\n",
    "\n",
    "|\t1|\t2|\t3|\t4|\t5|\n",
    "|--- |---|---|---|---|\n",
    "|0.1312|0.1656|0.1334|0.2363|0.1783|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "### Try to code the next nearest neighbor for the iris data set `iris_data`\n",
    "\n",
    "calculate_nearest_neighbor <- function(distance_matrix){\n",
    "    #' @param distance_matrix the distance matrix from above\n",
    "    #' @returns a vector of the next nearest neighbor for each sample\n",
    "\n",
    "    # here goes your code\n",
    "\n",
    "    return(nearest_neighbors)\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0.131192676604916</li><li>0.165588650131431</li><li>0.133389398453683</li><li>0.236318085072083</li><li>0.178312348246252</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.131192676604916\n",
       "\\item 0.165588650131431\n",
       "\\item 0.133389398453683\n",
       "\\item 0.236318085072083\n",
       "\\item 0.178312348246252\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.131192676604916\n",
       "2. 0.165588650131431\n",
       "3. 0.133389398453683\n",
       "4. 0.236318085072083\n",
       "5. 0.178312348246252\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.1311927 0.1655887 0.1333894 0.2363181 0.1783123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_nearest_neighbor <- function(distance_matrix){\n",
    "    #' @param distance_matrix the distance matrix from above\n",
    "    #' @returns a vector of the next nearest neighbor for each sample\n",
    "    \n",
    "    nearest_neighbors <- apply(distance_matrix, 2, sort)[2,,drop=FALSE]\n",
    "\n",
    "    return(nearest_neighbors)\n",
    "\n",
    "}\n",
    "\n",
    "dist_nn <- calculate_nearest_neighbor(dist)\n",
    "dist_nn[1:5]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High dimensional edge weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the high dimensional edge weights $w_{high}(x_i, x_j)$. As mentioned in the lecture we will not solve the following equation analytically for $\\sigma_i$ but use a binary search.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Large \\sum_{j=1, j\\ne i}^{k} \\exp(-\\frac{d(x_i, x_j)-d_{\\text{nearest neighbor}}}{\\sigma_i}) = log_2(k)\n",
    "    \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "Instead of calculating the sum of the high dimensional edge weights, we first calculate the high dimensional edge weights for a specific sample (and do not take the sum).\n",
    "\n",
    "Different from above equation we will use $k = \\text{total number of samples}$ (150 in our case), instead of $k$-nearest neighbors. This saves us some computational power by not sorting for the $k$-nearest neighbors for every sample (and later makes taking the \"fuzzy union\" of two edges easier). Therefore we will return all the high dimensional edge weights for a specific sample and not just its $k$-nearest neighbors high dimensional edge weights.\n",
    "\n",
    "Choosing a $\\sigma$=3 we expect the first 5 biggest high dimensional edge weights (after sorting them) for the first sample to look something like this.\n",
    "\n",
    "|\t1|\t2|\t3|\t4|\t5|\n",
    "|--- |---|---|---|---|\n",
    "|1.0000|0.9993|0.9815|0.9656|0.9582|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "high_dim_edge_weight <- function(dist, sample_index, dist_nn, sigma){\n",
    "    #' @param dist distance matrix\n",
    "    #' @param sample_index integer of sample, for which we want to \n",
    "    #'                     calculate the edge weights of its neighbors\n",
    "    #' @param dist_nn nearest neighbors\n",
    "    #' @param sigma decaying constant\n",
    "    #' @returns the high dimensional edge weights as an array for a specific sample. \n",
    "    #'          This should be 150 long vector\n",
    "\n",
    "    # here goes your code\n",
    "\n",
    "    return(high_dim_edges)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1</li><li>0.999268027406481</li><li>0.981538686508379</li><li>0.965565052069527</li><li>0.958205973697068</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 0.999268027406481\n",
       "\\item 0.981538686508379\n",
       "\\item 0.965565052069527\n",
       "\\item 0.958205973697068\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 0.999268027406481\n",
       "3. 0.981538686508379\n",
       "4. 0.965565052069527\n",
       "5. 0.958205973697068\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1.0000000 0.9992680 0.9815387 0.9655651 0.9582060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "high_dim_edge_weight <- function(dist, sample_index, dist_nn, sigma){\n",
    "    #' @param dist distance matrix\n",
    "    #' @param sample_index integer of sample, for which we want to \n",
    "    #'                     calculate the edge weights of its neighbors\n",
    "    #' @param dist_nn nearest neighbors\n",
    "    #' @param sigma decaying constant\n",
    "    #' @returns the high dimensional edge weights as an array for a specific sample. \n",
    "    #'          This should be 150 long vector\n",
    "\n",
    "    d <- dist[,sample_index] - dist_nn[sample_index]\n",
    "    d[d < 0] <- 1e10 #this sets the edge weight from the sample to itself to 0\n",
    "\n",
    "    high_dim_edges <- exp(-d/sigma)\n",
    "    high_dim_edges <- unname(high_dim_edges)\n",
    "\n",
    "    return(high_dim_edges)\n",
    "}\n",
    "\n",
    "# we calculate the edge weights for the first sample (sample_index=1)\n",
    "high_dim_edges_1st_sample <- high_dim_edge_weight(dist, sample_index=1, dist_nn, sigma=3)\n",
    "\n",
    "# and check if that agrees with the table above. the 5 highest edge weights are\n",
    "first_five <- sort(high_dim_edges_1st_sample, decreasing = TRUE)[1:5]\n",
    "first_five"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary search for sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the binary search we need to calculate the $k$-nearest neighbors as a function of the high dimensional edge weights or more specifically the $\\sigma_i$ sigma. Different high dimensional edge weights (or $\\sigma_i$) for a sample $i$ result in a different $k$. \n",
    "\n",
    "Find the mathematical expression that describes $k$ as a function of the high dimensional edge weights. For this you need to rewrite equation 1 and singulate the $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "k <- function(high_dim_edge_weights){\n",
    "    #' @param high_dim_edge_weights the high dimensional edge weights of a sample \n",
    "    #' @returns the expected number of k-nearest neighbors given \n",
    "    #'          edge weights (or more specifically sigma).\n",
    "    \n",
    "    # here goes your code\n",
    "\n",
    "    return(summation)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "k <- function(high_dim_edge_weights){\n",
    "    #' @param high_dim_edge_weights the high dimensional edge weights of a sample\n",
    "    #' @returns the expected number of k-nearest neighbors given \n",
    "    #'          edge weights (or more specifically sigma).\n",
    "     \n",
    "    summation <- 2**sum(high_dim_edge_weights)\n",
    "    return(summation)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing the binary search function, convince yourself that changing $\\sigma_i$ in the high dimensional edge weight function `high_dim_edge_weight()` and plugging the result into the function `k()`, will return a different result of k each time and therefore playing around with $\\sigma_i$ will eventually find the correct and expected number of $k$-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Plug the result of \"high_dim_edge_weight()\" into \"k()\" and \n",
    "# find the correct sigma for the first sample manually by playing around with the sigma value.\n",
    "\n",
    "# here goes your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "205.674971808173"
      ],
      "text/latex": [
       "205.674971808173"
      ],
      "text/markdown": [
       "205.674971808173"
      ],
      "text/plain": [
       "[1] 205.675"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6.28971077228494"
      ],
      "text/latex": [
       "6.28971077228494"
      ],
      "text/markdown": [
       "6.28971077228494"
      ],
      "text/plain": [
       "[1] 6.289711"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "14.4733783725479"
      ],
      "text/latex": [
       "14.4733783725479"
      ],
      "text/markdown": [
       "14.4733783725479"
      ],
      "text/plain": [
       "[1] 14.47338"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_nn = 15 # k-nearest neighbors = 15 is our goal\n",
    "\n",
    "k_approx <- k(high_dim_edge_weight(dist, sample_index=1, dist_nn, sigma=0.2))\n",
    "k_approx # too large\n",
    "k_approx <- k(high_dim_edge_weight(dist, sample_index=1, dist_nn, sigma=0.05))\n",
    "k_approx # too small\n",
    "k_approx <- k(high_dim_edge_weight(dist, sample_index=1, dist_nn, sigma=0.09))\n",
    "k_approx # close enough to our goal of k_nn = 15\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually write k as a function of sigma $\\sigma_i$ like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "14.4733783725479"
      ],
      "text/latex": [
       "14.4733783725479"
      ],
      "text/markdown": [
       "14.4733783725479"
      ],
      "text/plain": [
       "[1] 14.47338"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_of_sigma <- function(sigma){\n",
    "    return (k(high_dim_edge_weight(dist, sample_index=1, dist_nn, sigma)))\n",
    "}\n",
    "\n",
    "k_of_sigma(0.09)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the binary search function. Depending on your programming experience writing the binary search algorithm completely alone can be quite difficult. Feel free to google the binary search algorithm. Take care that you have a stopping criteria, i.e. a condition that stops your function past a numerically sufficient point of precision.\n",
    "\n",
    "For the first sample we expect a $\\sigma_i$ = 0.09 with $k$-nearest neighbors k_nn = 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sigma_binary_search <- function(k_of_sigma, fixed_k){\n",
    "    #' @param k_of_sigma k as a function of sigma\n",
    "    #' @param fixed_k our binary search goal. k_of_sigma should approximate this fixed_k at the end.\n",
    "\n",
    "    # here goes your code \n",
    "    \n",
    "    return(approx_sigma)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0925064086914062"
      ],
      "text/latex": [
       "0.0925064086914062"
      ],
      "text/markdown": [
       "0.0925064086914062"
      ],
      "text/plain": [
       "[1] 0.09250641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigma_binary_search <- function(k_of_sigma, fixed_k){\n",
    "    #' @param k_of_sigma k as a function of sigma\n",
    "    #' @param fixed_k our binary search goal. k_of_sigma should approximate fixed_k at the end.\n",
    "\n",
    "    sigma_lower_limit <- 0; sigma_upper_limit=1000\n",
    "    for (i in 1:20){\n",
    "        approx_sigma <- (sigma_lower_limit + sigma_upper_limit) / 2\n",
    "        if (k_of_sigma(approx_sigma) < fixed_k){\n",
    "            sigma_lower_limit <- approx_sigma\n",
    "        }\n",
    "        else{\n",
    "            sigma_upper_limit <- approx_sigma\n",
    "        }\n",
    "        if (abs(fixed_k - k_of_sigma(approx_sigma)) <= 1e-10){\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "    return(approx_sigma)\n",
    "}\n",
    "\n",
    "k_nn=15\n",
    "sigma_approx <- sigma_binary_search(k_of_sigma, k_nn)\n",
    "sigma_approx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same for the rest of the samples (not just the first one) and save the $\\sigma_i$ values in a vector.\n",
    "\n",
    "The first five $\\sigma_i$ values of the first five samples should be something like:\n",
    "\n",
    "|\t1|\t2|\t3|\t4|\t5|\n",
    "|--- |---|---|---|---|\n",
    "|0.0925|0.0887|0.1383|0.0639|0.1249|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also populate a 150x150 matrix with all the high dimensional edge weights. For each sample all the high dimensional edge weights go into one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary search progress: processed sample 50 out of 150\n",
      "binary search progress: processed sample 100 out of 150\n",
      "binary search progress: processed sample 150 out of 150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 5 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0.000000e+00</td><td>1.176261e-05</td><td>0.005916391</td><td>1.347909e-06</td><td>5.230809e-01</td></tr>\n",
       "\t<tr><td>1.295095e-05</td><td>0.000000e+00</td><td>0.060352539</td><td>4.636926e-02</td><td>6.548819e-05</td></tr>\n",
       "\t<tr><td>4.563063e-04</td><td>1.805524e-02</td><td>0.000000000</td><td>4.820527e-01</td><td>1.528988e-03</td></tr>\n",
       "\t<tr><td>2.829385e-05</td><td>4.929161e-02</td><td>0.339083993</td><td>0.000000e+00</td><td>1.943044e-04</td></tr>\n",
       "\t<tr><td>2.504424e-01</td><td>1.107476e-06</td><td>0.002066182</td><td>1.372178e-07</td><td>0.000000e+00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 5 of type dbl\n",
       "\\begin{tabular}{lllll}\n",
       "\t 0.000000e+00 & 1.176261e-05 & 0.005916391 & 1.347909e-06 & 5.230809e-01\\\\\n",
       "\t 1.295095e-05 & 0.000000e+00 & 0.060352539 & 4.636926e-02 & 6.548819e-05\\\\\n",
       "\t 4.563063e-04 & 1.805524e-02 & 0.000000000 & 4.820527e-01 & 1.528988e-03\\\\\n",
       "\t 2.829385e-05 & 4.929161e-02 & 0.339083993 & 0.000000e+00 & 1.943044e-04\\\\\n",
       "\t 2.504424e-01 & 1.107476e-06 & 0.002066182 & 1.372178e-07 & 0.000000e+00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 5 of type dbl\n",
       "\n",
       "| 0.000000e+00 | 1.176261e-05 | 0.005916391 | 1.347909e-06 | 5.230809e-01 |\n",
       "| 1.295095e-05 | 0.000000e+00 | 0.060352539 | 4.636926e-02 | 6.548819e-05 |\n",
       "| 4.563063e-04 | 1.805524e-02 | 0.000000000 | 4.820527e-01 | 1.528988e-03 |\n",
       "| 2.829385e-05 | 4.929161e-02 | 0.339083993 | 0.000000e+00 | 1.943044e-04 |\n",
       "| 2.504424e-01 | 1.107476e-06 | 0.002066182 | 1.372178e-07 | 0.000000e+00 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]         [,2]         [,3]        [,4]         [,5]        \n",
       "[1,] 0.000000e+00 1.176261e-05 0.005916391 1.347909e-06 5.230809e-01\n",
       "[2,] 1.295095e-05 0.000000e+00 0.060352539 4.636926e-02 6.548819e-05\n",
       "[3,] 4.563063e-04 1.805524e-02 0.000000000 4.820527e-01 1.528988e-03\n",
       "[4,] 2.829385e-05 4.929161e-02 0.339083993 0.000000e+00 1.943044e-04\n",
       "[5,] 2.504424e-01 1.107476e-06 0.002066182 1.372178e-07 0.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0.0925064086914062</li><li>0.0886917114257812</li><li>0.138282775878906</li><li>0.0638961791992188</li><li>0.124931335449219</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.0925064086914062\n",
       "\\item 0.0886917114257812\n",
       "\\item 0.138282775878906\n",
       "\\item 0.0638961791992188\n",
       "\\item 0.124931335449219\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.0925064086914062\n",
       "2. 0.0886917114257812\n",
       "3. 0.138282775878906\n",
       "4. 0.0638961791992188\n",
       "5. 0.124931335449219\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.09250641 0.08869171 0.13828278 0.06389618 0.12493134"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_nn <- 15\n",
    "n <- dim(iris_data)[1]\n",
    "high_dim_edge_weight_matrix <- matrix(0, nrow=n, ncol=n)\n",
    "sigma_vec <- rep(0, n)\n",
    "\n",
    "for (sample_index in 1:n){\n",
    "\n",
    "    binary_search_result <- sigma_binary_search(function(sigma) k(high_dim_edge_weight(dist, sample_index, dist_nn, sigma)), k_nn)\n",
    "    \n",
    "    high_dim_edge_weight_matrix[,sample_index] <- high_dim_edge_weight(dist, sample_index, dist_nn, binary_search_result)\n",
    "    sigma_vec[sample_index] <- binary_search_result\n",
    "    if (sample_index %% 50 == 0){\n",
    "        cat(paste0(\"binary search progress: processed sample \", sample_index ,\" out of \", n, \"\\n\"))\n",
    "    }\n",
    "}\n",
    "\n",
    "high_dim_edge_weight_matrix[1:5, 1:5]\n",
    "sigma_vec[1:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging/fuzzy union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the fuzzy union operator \"to average\" two different high dimensional edge weights, e.g. like in the lecture $w_{high}(B,C)$ and $w_{high}(C,B)$.\n",
    "\n",
    "The formula was given by:\n",
    "\n",
    "$\\text{fuzzy union}(x,y) = x + y - x*y$\n",
    "\n",
    "where $x$ and $y$ are the different edge weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Write the fuzzy union operator for the high dimensional edge weights \"high_dim_edge_weight_matrix\" from above.\n",
    "# Try to make use of matrix operations like transposing and multiplication instead of using loops. \n",
    "# Take care however to use elementwise multiplication and not matrix multiplication.\n",
    "\n",
    "# here goes your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 5 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0.000000e+00</td><td>2.471341e-05</td><td>0.006369997</td><td>2.964172e-05</td><td>6.425217e-01</td></tr>\n",
       "\t<tr><td>2.471341e-05</td><td>0.000000e+00</td><td>0.077318097</td><td>9.337526e-02</td><td>6.659559e-05</td></tr>\n",
       "\t<tr><td>6.369997e-03</td><td>7.731810e-02</td><td>0.000000000</td><td>6.576803e-01</td><td>3.592011e-03</td></tr>\n",
       "\t<tr><td>2.964172e-05</td><td>9.337526e-02</td><td>0.657680317</td><td>0.000000e+00</td><td>1.944416e-04</td></tr>\n",
       "\t<tr><td>6.425217e-01</td><td>6.659559e-05</td><td>0.003592011</td><td>1.944416e-04</td><td>0.000000e+00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 5 of type dbl\n",
       "\\begin{tabular}{lllll}\n",
       "\t 0.000000e+00 & 2.471341e-05 & 0.006369997 & 2.964172e-05 & 6.425217e-01\\\\\n",
       "\t 2.471341e-05 & 0.000000e+00 & 0.077318097 & 9.337526e-02 & 6.659559e-05\\\\\n",
       "\t 6.369997e-03 & 7.731810e-02 & 0.000000000 & 6.576803e-01 & 3.592011e-03\\\\\n",
       "\t 2.964172e-05 & 9.337526e-02 & 0.657680317 & 0.000000e+00 & 1.944416e-04\\\\\n",
       "\t 6.425217e-01 & 6.659559e-05 & 0.003592011 & 1.944416e-04 & 0.000000e+00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 5 of type dbl\n",
       "\n",
       "| 0.000000e+00 | 2.471341e-05 | 0.006369997 | 2.964172e-05 | 6.425217e-01 |\n",
       "| 2.471341e-05 | 0.000000e+00 | 0.077318097 | 9.337526e-02 | 6.659559e-05 |\n",
       "| 6.369997e-03 | 7.731810e-02 | 0.000000000 | 6.576803e-01 | 3.592011e-03 |\n",
       "| 2.964172e-05 | 9.337526e-02 | 0.657680317 | 0.000000e+00 | 1.944416e-04 |\n",
       "| 6.425217e-01 | 6.659559e-05 | 0.003592011 | 1.944416e-04 | 0.000000e+00 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]         [,2]         [,3]        [,4]         [,5]        \n",
       "[1,] 0.000000e+00 2.471341e-05 0.006369997 2.964172e-05 6.425217e-01\n",
       "[2,] 2.471341e-05 0.000000e+00 0.077318097 9.337526e-02 6.659559e-05\n",
       "[3,] 6.369997e-03 7.731810e-02 0.000000000 6.576803e-01 3.592011e-03\n",
       "[4,] 2.964172e-05 9.337526e-02 0.657680317 0.000000e+00 1.944416e-04\n",
       "[5,] 6.425217e-01 6.659559e-05 0.003592011 1.944416e-04 0.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "high_dim_edge_avg <- high_dim_edge_weight_matrix + t(high_dim_edge_weight_matrix) - (high_dim_edge_weight_matrix * t(high_dim_edge_weight_matrix)) #elementwise multiplication not vector product!\n",
    "high_dim_edge_avg[1:5,1:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low dim. Weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the low dimensional weight function `low_dim_weight()`. The formula was given in the lecture as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Large w_{low}(y_i,y_j) = (1 + a(|y_i - y_j|)^{2b})^{-1}\n",
    "    \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "with $a = 1.93$ and $b=0.79$.\n",
    "\n",
    "For `y <- c(1:5)` the `low_dim_weight()` should return for the first sample:\n",
    "\n",
    "|\t1|\t2|\t3|\t4|\t5|\n",
    "|--- |---|---|---|---|\n",
    "|1.0000\t|0.3413\t|0.1477\t|0.0837\t|0.0548|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "low_dim_weight <- function(y, a=1.93, b=0.79){\n",
    "    #' @param y low dimension weights vector\n",
    "    #' @param a parameter constant\n",
    "    #' @param b parameter constant\n",
    "    #' @returns low dimensional weights as a nxn matrix, where n is the total number of samples.\n",
    "\n",
    "    # here goes your code\n",
    "\n",
    "    return(low_dim_weight_matrix)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 5 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1.00000000</td><td>0.34129693</td><td>0.1477079</td><td>0.08368293</td><td>0.05479178</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0.34129693</td><td>1.00000000</td><td>0.3412969</td><td>0.14770791</td><td>0.08368293</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.14770791</td><td>0.34129693</td><td>1.0000000</td><td>0.34129693</td><td>0.14770791</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.08368293</td><td>0.14770791</td><td>0.3412969</td><td>1.00000000</td><td>0.34129693</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.05479178</td><td>0.08368293</td><td>0.1477079</td><td>0.34129693</td><td>1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 5 of type dbl\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & 1 & 2 & 3 & 4 & 5\\\\\n",
       "\\hline\n",
       "\t1 & 1.00000000 & 0.34129693 & 0.1477079 & 0.08368293 & 0.05479178\\\\\n",
       "\t2 & 0.34129693 & 1.00000000 & 0.3412969 & 0.14770791 & 0.08368293\\\\\n",
       "\t3 & 0.14770791 & 0.34129693 & 1.0000000 & 0.34129693 & 0.14770791\\\\\n",
       "\t4 & 0.08368293 & 0.14770791 & 0.3412969 & 1.00000000 & 0.34129693\\\\\n",
       "\t5 & 0.05479178 & 0.08368293 & 0.1477079 & 0.34129693 & 1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 5 of type dbl\n",
       "\n",
       "| <!--/--> | 1 | 2 | 3 | 4 | 5 |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 1.00000000 | 0.34129693 | 0.1477079 | 0.08368293 | 0.05479178 |\n",
       "| 2 | 0.34129693 | 1.00000000 | 0.3412969 | 0.14770791 | 0.08368293 |\n",
       "| 3 | 0.14770791 | 0.34129693 | 1.0000000 | 0.34129693 | 0.14770791 |\n",
       "| 4 | 0.08368293 | 0.14770791 | 0.3412969 | 1.00000000 | 0.34129693 |\n",
       "| 5 | 0.05479178 | 0.08368293 | 0.1477079 | 0.34129693 | 1.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "  1          2          3         4          5         \n",
       "1 1.00000000 0.34129693 0.1477079 0.08368293 0.05479178\n",
       "2 0.34129693 1.00000000 0.3412969 0.14770791 0.08368293\n",
       "3 0.14770791 0.34129693 1.0000000 0.34129693 0.14770791\n",
       "4 0.08368293 0.14770791 0.3412969 1.00000000 0.34129693\n",
       "5 0.05479178 0.08368293 0.1477079 0.34129693 1.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "low_dim_weight <- function(y, a=1.93, b=0.79){\n",
    "    #' @param y low dimension weights vector\n",
    "    #' @param a parameter constant\n",
    "    #' @param b parameter constant\n",
    "    #' @returns low dimensional weights as a nxn matrix, where n is the total number of samples.\n",
    "    \n",
    "    low_dim_dist <- as.matrix(dist(y, method = \"euclidean\", diag = TRUE, upper = TRUE))\n",
    "    #low_dim_weight_matrix <- (1 + a * low_dim_dist**(2*b))**(-1)\n",
    "    low_dim_weight_matrix <- (1 + a * low_dim_dist**(2*b))**(-1)\n",
    "\n",
    "    return(low_dim_weight_matrix)\n",
    "}\n",
    "\n",
    "y <- c(1:5)\n",
    "low_dim_weight(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "The low dimensional embedding can be generated in python with a few commands, whereas in R it is more complicated. Therefore when using the iris data set we will simply load the result of the python script, which looks like this:\n",
    "\n",
    "> \\# python code \\\n",
    "> import numpy as np \\\n",
    "> from sklearn.manifold import SpectralEmbedding\n",
    "> \n",
    "> model = SpectralEmbedding(n_components = 2, n_neighbors = 15) \\\n",
    "> y = model.fit_transform(iris_data) \\\n",
    ">\n",
    "> np.savetxt(\"./python_y_embedding_n_neigh=15.txt\", y)\n",
    "\n",
    "In R we can then load and plot it, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title \"2D spectral embedding\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y <- read.csv(\"../data/python_y_embedding_n_neigh=15.txt\", header=FALSE)\n",
    "y <- as.matrix(y)\n",
    "\n",
    "plot(y[,1], y[,2], pch=19, col=factor(iris_species), main=\"2D spectral embedding\", xlab=\"\", ylab=\"\", cex=0.5, cex.main=2)\n",
    "legend(\"bottomright\", legend=levels(iris_species), pch=1, col=1:3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also just use a random initalization if we want to, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1h0E9oaGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnfU2vh4eHp6enw8PD///+JrwZJAAAACXBIWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO3diXqiTLtGYWinGBOH8z/ZjoAKMhTgU8Vbxbqvvb8/sVWKwIqASLIbgI9lSw8ASAEhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECKwipNN+k2XZ7uvyvCWr2+y+fnwPoZxS4Kfsu4fzkbU7fD5wD7NuUPIzeLt9589k9o/bsncbzylZCOm8G/tIRUjjp5aE5Gfwdqj3sq1ubIWUZV9eR7F8SOfd88sAIU2ZWhKSn8HvZi7Vr8mOkLKDz2EsHdJ9xQ4X0rSpJSH5GXxt15VOxa1dIWU+t+6WXpumTf/T0S49twtIfW5P5TL9/vuqTKrcTaov6ctpV36bexzH0qsWIXmW+tzui0Va7P8ca7W8Lekqt6O/cSy9ahGSZ6nP7aZYpOf7l9eBLf8yso2/cSy9ahGSZ6nP7XG/222qmRwIqR5cXXW34/avskP5rz+HYkswf70t9Xiu0z5/3a1w/fp7YH6/5X2Cl6/iWVrvbT2fZl/usZ0Pm/s3rYE15qFvAI97ZHWtuW/PUPdPKnvT9/jhqbnn/NA1s+alHlJdsby2ry/r8/5V3NDativvti3vfn/Bumxqa8m+fqefzdvNz92y+6Zlc4Ln3etJds0ef55HR+4HGHf1b7rGVgupPYDHPYZW7a4ZmhJS+/FDUxuc8/YPMCIrCumnWEjlQe7Gwn39Y2sJlrdWy/f7b7VpHgSsVXmq3Vyt9cfXLYfGBGv/UDy0Nq36u17b+tTaJb2esm8Aj3s0ptac+84ZmhBSx+MHpjY05x3jj8mKQip/F5bbDfWFW7h0L8DGYr8+tgBfvlt3eq0gP+3byyc9dt69+5B8a6V7H9vAAMaE1DlDE0LqePzA1EbPeWtmzVtPSOVvvLff1S/tW163lvbVu7v7vy2Sa7XVtqvd6X6W0ffzro91LD/VNvGK5zyXX+/+7v5Tbeic60/z94Dz4wG1R3e/WmYDA+jdKXx90z1Djjdkyy3dw7jH179xzfn7+GOympCqbZBqs7y9enSsMM9bs221/7t5LeOyy83rTuVx9XJf676FU70gFRO81EPavNbDxwlMraeptnPyy9ukukfcPYAxIXXP0HBI29oU3I9/n5przuvjj8laQqo6ehxNmBbS853a6+mwubQekNWe+vy8+au91hT3/mlmUa5cP29PUx2qL8//uwyNrX8AY0LqnqHBkA71H4n78bVvRs35uTXBOMQ34lmqjp6n000LqfMsvPf1+Px28+65ntyeL0/3L8sV8fvxNN/PKXQ8zftzThrAmJB6fgIDIVWvlZf3R4+Y2pQ5bz+9cfGNeI73jiaG1D4J7+dr+74evz9PXr+19mbwtrkili832+6nubrH1j+ASSE1ZmggpOrVsXUwoPfxtW+mzPn785sX34hnqDqqfVCitbReS7WhvF/jHcKf4652yLf1ZM9vmpPouXfvA0asW6qQ2jM0ENL7byTn43t/IqPmPB7xjXi6qqP6b9HW0io3zXsOf9eeqvHppsH1uPnQnnv3PmDEuuUawKiQOmeoP6T6gYZxjyekZJQd5Y2t+tbSKt/h6D6z4XW/x5uG28N3bWNtzGrTc+/eB4xYt1wDGBNS9wz1PrA60HB1/EAIKUnFb9Ftc++4tbTKX7WtnaHm/cqXrfz09k/d60F5UKpa52buI/WNdtQARoTUM0N9D6yyeW3pjng8+0ipKBbf+5sw70urWiNaD27eb19fj1zr8cyjdl0T9hZSzwz1fFkdaHgOfdTja9/MmPN4xDfiicpttvejtW9L6+1dpr771b+rvd/RvR681pNb432kt7dXX6edLxBSzwz1PLB9oGHE42vfzJjzeMQ34mke5xS8FDc3ltb5q1xFOj4h21yq2XPRP7ZTBtbjamuuSPhcn3bH+/u7/qdpj6I9tr47O1ftnhnqfuD2NVTHD6RvQNPnPB7xjXia2ln79ZW5detdx5mSzaVanT13P1VsW3u2nvWg2jNvnWtXbebVzzi7DDxNaxTtsfXdufZlOfTv26XxCtAzQ53PUc5O8w2Cvsd3T23GnMcjvhFPcs1aitvbN3efv9Bcql/tB13f7/T65vp+3ZXHfd6PGGetnXV9SM9fKI19kp4Z6nyO9l37H989tRlzHo/4RjxJ67z9/pA6r2v3tlRraeTlr+ChTfz6Z3UO9fs017/81DGtEevW69a+O9e+fH7aZ9e4vXuGRofU9/ieqU2f83jEN+JJ2lt2PSHtuj/f/LZUL48NmGxfHUw4vt+p/s3r06PVR2Qfz9P4nOi1a1oj1q3XrX13rt/+mGLeuL17hsaH1PP4nqlNn/N4xDfiSdpLvyOkze7YcQ5m7X61G067x2UFyq1Gx9sgxd23X9fb7e2JLuV5Ne3rkTu/6bh/350btxcXHdvsv99u75qh8SH1PL53alPnPB7xjRgwiJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJEAgQEgdfzUPMG3GWq4PZ4FJjGdqMDCKkJxMDQZGEZKTqcHAKEJyMjUYGEVITqYGA6MIycnUYGAUITmZGgyMIiQnU4OBUYTkZGowMIqQnEwNBkYRkpOpwcAoQnIyNRgYRUhOpgYDowjJydRgYBQhOZkaDIwiJCdTg4FRhORkajAwipCcTA0GRhGSk6nBwKgEQzqpRlEhJLilF9JGPb7VhDTrCh4oxRLS+IUsXxvWsnbNvBYOCnGEVCzgkUuZkGYipE9EElLPhE/bLNuW+0THTZYfb7XV4e+GzfHtXqddluWHDweTKkL6RBQhZd03347lpfnuueyKr7av1WH7uKF2r6/yq4klrWbloqMPRB1Snp1vt+9s8/dSk22vt+s2Oz027b6z/Hw759l3/V7Z/dvvqasLaxfcog4pyx6HunfZ9e+/12z3CGlX/NOpfI06vT3qo8EAHaIIqW8f6ZBlu/O5+KfXFZjLTqpa7v/zutftdjl9bQkJepGE1HPU7iv/aye/DIf0ule140RIkIsjpP4d4dNhU+791O75+u/jf6p77bPN8XQhJOjFEtLQnbPHLtHz29c+0q5+c/FP8YX079+/pYcAl6hD2pRH4TbVQbrbsTzYcN+Iqx21e90ry35u5+j2kf79oyT7og7pu9zl+bk99n7uO0Kbv/+51d9Het3rkL0eIB+MN4QUg6hDKs9ZKLM4/gW0v78W/WyKkG7HvHFmQ3Gv/f2L2vaedDC+EFIM4g4piMUHQ0cRICQnU4OBUYTkZGowMIqQnEwNBkYRkpOpwcAoQnIyNRgYRUhOpgYDowjJ6fkhDj73hl6E5FQNhk9iYwAhORES3NIPqXftH5sFIcGNkNxP8Lw/HaFPLCH9/v7KJzIxJKBfHCEVFalTIiToRBJS7b9P1/tn9W73DyBdX9eH/Mvjurl/TuJ5Vciyl0OebS/F3Z8Xjny7juTjgc7BAG1RhPT79r+VbfFZ2Mv903vP60P+9bC7XwLydVXI1+Ui8+ut/oG/t+tIVg90DgboEHNI39nX7X751FPz+pDbey/1q0Lev/i7cX/PpPYR9PIfat8WD3QOBugQc0i3Yttuk71dH7L4LOzrqpDltVF+7v+cv1048u3bnk+gExLcogipex/p/snxy9+W3eHWvqxd49qR9eMKtct01f6heR/HYIC2SELqPmr387dtd7i/kLRDql87kpDgXRwh9b2PlG/u/9eI4PVl7dqRhATPYgmp2yE7FgccWteHrH1z//9tax9p19xH2hESPhJ3SJe/zbf7YYbG9SGLf6lfFfL+L9vrX3WOo3YfDgZrFndIf71si/99XR+y6uF1Vcjx7yN9OhisWOQhfT+26Z7Xh3z08LwqZHVmQ5btqjMb8uaZDc1vPxkMVizykEIwNRgYRUhOpgYDowjJydRg1Dx8PGWdCMnJ1GDEfn8pSYOQnEwNRoyQVAjJydRgxAhJhZCcTA1GjY5ECMnJ1GBgFCE5mRoMjCIkJ1ODgVGE5GRqMDAqjZC6z5Nr3jr78o6EBDdCkg8GaxRLSAv+ZW9CglscIRUVLZUSIcEtkpBq/32qXWm1/Eh5daXUQ54dahdluOyy/P5x9Lcrrp52WZZ3XQ/SORigLYqQ/r39b+V1pdUymfJKqcVnXvevkO5XE7pf2KH5Sdmv8hO0Y0oiJLjFHNLrSqtlMsWVUk/VVRieIf3demxfcTUrr+kwZl4ICW4xh/S60mqZTHGl1Md1gRq3Pr57XXG1egJCgkbIkK776g9E3Fyr8Kh9pMaVVp/P2HnhurfL2BUup68tIUEkYEjXYm+l+sspE0PqPmpXu9Lq5JC2j0uzOhES3AKGdLj/kZXrMS8uoDUtpL73kepXWp0W0j7bHE8XQoJIwJDy8oGXfHOZHlK3x5VWa5G09pFutX9/XXG1uJ2QoBIwpMdae91uu0LK6kY+5eNKq7WQWkftbrV/f11x9X4Q4sw+ElQChlT8icryq63oFelxpdX6Ztu28YcpmiG93kc6ZM9LsboQEtwChnTM9tVXl8zxWjB6EtWVVhv7P/eTF356QnpdcfV+BPHn1PlHY+cOBisW8vD34bmunxxbb5+vu9U1wRUICW5B35A9P3//X/a+QipOWbjuRp38M/IZZc+EdEVxZsMU1Ul0ufueAQaD1UgupNvxb/9ro3s9IiSMkV5IcqYGA6MIycnUYGAUITmZGgyMIiQnU4OBUYTkZGowMIqQnEwNBkYRkpOpwcAoQnIyNRgYRUhOpgYDowjJydRgYBQhOZkaDIwiJCdTg4FRhORkajAwymhIpvifX0TPaEj+J2HJgn+zBiKEtLx//ygpeoS0PEJKACEtj5ASQEgG0FH8CAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJEzEeRhdCAnTcGZgJ0JC5ff3d8zdCKkTIaH0+zuuJELqREgojQ0p++uI5dNCSBZY+B0/OiQuY9GFkAywsbU0ch+JkDoRkgE2QhqLjroQkgFxhVQX67j1CMmCWNfHeH8DyBES5iOkJ0LCfIT0REjrM/Lw3Bh09EBIqzP2DSNMQUhxETRASD4QUlQUERCSD4QUFUkEdOQBIUWFVxOrCCkudGQUIQEChAQIENKiVnsmdXIzTkhLWu1ne9KbcUJaUnrr00jpzTghLSm99Wmk9GackBaV2uo0WnIzTkiAACEBAoSEMfjgkQMhYQQ+CutCSBiBkFwICSMQkgshYQw6ciAkQICQAAFCAgQICV2SO4XHN0JCh/ROKvWNkNCBkKYiJHQgpKkICV3oaCJCAgQICRAgJECAkAABQsIq+D7rlpCwBt4/B0JIWANCAqbp/IMdhARM0vMnpNhHwiSr/wNKC/0tNkJKC3/Sj5DCTiJRhLTQazIhpYWQFkJIiVmiIy4xREj4HBe9uxFS/JbflCOkGyFFz8BOESHdCCl6BkJiH+lGSNGzEBIIKX6WO2pe+cHySD9GSJho/HVRmtciSvu1k5AwzYQrdRGS+iEGJ4GZlg/J5LGNgCFlTT4mAf+mXDvSyz6SzaPtAUM6Doc0ujIsa+nFs/qQbud863sSSB8h3c7ZwfckkD6LHQU+2HDMzr4nASyBo3aAACEBAoSUKpN7EukipETZPLaVLkJK1MpCWnxmCSlR6wpp+bklpFQtvWYFRUiLTQLmfNACIS02CVjzUQxLd0RIcFN/jqj7tNflX1U+QUhwUX8ir+f8fkLSIyRLAoW0/ObZJwgJLqFCihohrcycX/th9pHiRkjrEveOiGGEtC6E5AkhrQsheUJIK0NHfhASIEBIgAAhYRbFJmJKlzAmJMyhOGiR1MXACQlzENIbQsIchPSGkDCLgX0kU2caERIi9X7u67Ivb4SESL2FtPCGIiEhUoRkYhKIXmvLjpAWmARSwz7SIpMAlAgJECAk4xJ6zzJphGRbUu/+p4yQbCOkSBCSbYQUCUIyzt2RtrQoujU4SEKKnfY1K4pXQIuDJKTYEZIJhLS8zz4OQEjhDCwpQlrcp5fCTmcfafTPYbmOekdISItb6Jryj0/m2bnQnfmL6xOSaZPWH9lq//isuKFLrxKSnOkfp9y0jt5X+5kr37IhdW6bmQ+JfaRktFf7eWvf7++iIfUcLbDe0RBCiooopPt6vOQ+UhTHBqchpLh0bdnNC2nJFXnp6XtASLGbt2W38IqcXEeEtE7prchLIyRAgJAAAULCg7/jdyvYkiSkpIw58tDXy8R3lCYc5Fj82EYAhGTJp+9IjjkW3tvLtJCmHHYnJNVDDE7Coo/PkSGkxRCSIcuG5NhHenvmSWNNvyNCsuTzszY/2UdyPnOrpBlPM2vCQabzIUKyxPBKs9Sp2fZPCS8REkbpXqH9b7MR0idi+MmtTU9HvksipE/E8JNDmMNxcXRESPjAGo5rj0RIk0XyKzIIOnogpKli2WhHUIQ0FSGhAyFNZTsky2NLGiFNZnldfVZueZBJIqSkPEKy/bKZIkJKCiEthZDS8tqyI6SgCMm+Oadr01FghGTC0Bubhi5zj16EZMHgqTbRhBTJMP0gJAuSCCmWcfpBSBYMn/wZYv0UTIOQ/D/E4CRsWfrkT0UEqpCW/lnMQ0i4iSIY8RQj7hLpRzMICbdgm2VjJkNIQoQUWpjdG0L68CGqSazrbUbh3Eo7mT+uUS98UXYUV0jrOvFFOLfSLbdPxpXucT1CMivFkNJFSGYRUkyiCol9pLkUHT2H42cpRL5s4woJy/H8QhT76xwhYRxCGkRIGIeQBhFSUnweXva8osfdESElZd0nYC+KkFIiDSnOMwyWQkgpUYY06py3yLfHhAjJqlkvCFVHiheTZkjdxcR+hEAoaEg/X7viR787/PiaRDIGXxAcoUhOoG48SU8xhPQUMKTrJnvZepmEhomVYygGVyiEFF7AkA5Z/n0uvrqc8uzgYxISNtaOj0P6NKUxIdn4nWNCwJDy7Pz8+pzlPiYhYSOkoc03ZyaKksbsI+EpYEiNRdFeLlndzElILD4AN3ck6pckOPCK1DF16x2NoKiAkiYIu490uhRf2d5HSoP6cAOGhTz8va1tu22uXiaxRt7W9jEhcUpSJez7SIfifaR898X7SDIeXzdGdURJBc5siN2iG2CE9EBIsfMW0pinJaQHQoqev44G/0LG2xdrR0hrNtTgUEi8ELUQ0oo9W+lKxltIKbxN10ZIK/ZopbsZT3/7LIITR+YgpBUbDmnIZy9IhDT3IQYngdvzRSfoEXRC+uAhBieBmqDvRCXZESEBCoQEvTRfdAYREuQS3Q0aREhxMv0BB0Ly9RCDk4ib7Y8KEZKvhxicRNxsh/S+j7SGrAgpSsZDalrFCxQhxSmijghJ9xCDk0AwhCR7iMFJIJR///6tYJeJkOBX60zxNF+gCCkq5ToZ0w4SISkfYnASUSpXSgOH7CZ8jIKQhA8xOIkoWQlp0gf7WndNsSNCikqUIa0DIUXFyD4SIbUQ0rqINqvo6B0hxYcLJhhESNHhEj4WEVJ0CMkiQorO4ldnZAepAyHFZ+EVmUN2XQgJExFSF0LCRITUhZAwFR11ICRfODy2KoTkCQea14WQPCGkdSEkTwhpXQjJF2lHRGkdIQU18wMQvLyZR0ghzf1IHiGZR0ghhQxp8U//DbI9uhkIKaTZHxKf1ZHhddX26OYgpKDCrT4GVtWBMyAMjE6MkBK1/Ko6dE7e8qNTI6RUFWvqkqvr4MmtqXVESEl7/uJf4jzTdZ0lTkgpe4S0zDq9po4IKWnLhrQqhJS015ZdQiGZ3L8ipFVIrCODJRESIkNIpiaBWBGSqUkgWhY7IiRAgZBgRdRHRAgJRsR9jJ6QYAQh6SUSUswrxkjCWSQkvTRCinvNGEU6i1H/tAjJH0JaEULyx9Nattiq2/F5d0J6ICSPfHW0zLrbeQUWOqoQUmyChNR18gDXBBtCSLEJEVLn6WzNkGZHlehrGCFFJ8wLUvdLUv3reSWluldFSGhzn2BNSG8ICR2cJ1gT0htCwizsIzURUsoSXWktIqSEpboZ5clHPyxCShghTfHZT4uQggr7KWnzIZl6h5eQvIzAyyIOfd0O+x0ZKomQfAzAzyK2eQGcxdgKiX0kHwMgpACMhfQRQuoegKdF3N9ROqvUBMVMp/G7hZB6RhB4vZ5ZrvGdoDEmv0rbDI+QbJgXkvnDciNMDcno5jEh2WA0pAAvzITk0fpCmrfK+g4pyNGA6Vt2hGRoEmnw/4Jk7xiIyY4ICQNMhmQTIWEAHY1FSOg34hWJ1EqEhH7ukNj4qxAS+hHSaISEAX2VPG8npAohYbpaPnRUIiRMx+tQS8CQsiYfk0AYhNQSMKQjISWDjt6F3LQ751vfkwCWEXQf6ZwdfE8CWETYgw3H7Ox7EsAS7By1G70DtQI2T3CeK6256WEnpMCTsEz/kRvfH6QdGq/RDxCJEZJB8lWv+QFAfVWDA549N1H1t0RI7i03QvIYkoeP1XoJKa5XMkKyyMOW3XIhzZ0bQnI+ASGF9rZl52PbTvyMhDTiCQhpUR911P9gH69z2if0iZAwRf/LWQrX2PsAIWEKQurB4W9MQUg9CAmTBNxHigohAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASVKL62IMaIUEkrg/iqRESRAjJ/0MMTgJqhOT/IQYnAbk1d0RIgAIhAQKEBAgQEoyJ8yPrhJQke3/QY3QekV5EhZBSZO9P44zPg5CEbK0E8SGk4AgpRTGHxD6SkK2VIELWOoo1j/EICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQsJsqX/GaApCwlyRfijcD0LCXIRUQ0iYi5BqCAmz0dELIcHJ3qVU7CEkuNi7uJdBhAQXQhqBkOBCSCMQEpzoyI2QAAFCAgQIafVW/TeUZQhp7X5/KUmAkNaOkCQIae0ISYKQVo+OFAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCghkxX0uckGBF1H/dgpBgBSHJEdIaEZIcIa1SxB0REqBASIAAIQEChAQIEBIgQEiAQMiQrvss256qJxl8FkJCZAKGdM2LP0a6K5+EkJCSgCEdsuNfTcd8WzwJISElAUPKywde8s2FkJCYgCE92rlut4SExAQMaZNdH19tCQlpCRjSMdtXX12yLSEhKSEPfx+e9ZwyQkJSgr4he949vrrsW8+S1c2eBLAIzmwABAgJEFgiJPeWGyEhMoQECBASIEBIKeP4ZzCElDDeSQiHkBJGSOFw+DthhBQOIaWMjoIhJECAkAABQgIECAkQICRAgJAAAUICBAgJqQvybhohIXFhzu8gJCSOkAABQgIU2EcCYkFIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgsEBIxzzbHP1OAggsZEjnXZYfb1/Z3dbPJIBlBAzpXBR0yPbX22WXDb4mERIiEzCkfXa43Q5Zfv/6mm18TAJYSMCQsuKB2a72jXoSwEKCh/RdbtOVL0zqSQALCbpp97d3VLoWm3n6SQALCRjSNX9uz2XDL0iEhNgEfR/p8Mgn73g9yupmTwJYBGc2AAKEBAgsEZJ7y42QEBlCAgQICRAgJECAkAABQgIEOPwNCBCSHb+/v0sPAXMRkhm/v5QUL0Iyg5BiRkhmEFLMCMkOOooYIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgIDRkIDIzFjL9eH4Em6owabEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhjMkwWXEhGKY0ijGhgPEiZAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJEIgopOs+y/bnIJM6brL8cA0zKf9L4JCnNDfldMItoJEiCikv/k5AiJIOxZTyEAvqPOcvH0yzLeZm43sydwHmphBwAY0VT0iHbH//z87/lM7Z/nr/5boPMKnc+6r3k+Xn+3R+PE/nFmRuyumEW0CjxRNSnt1/AYVYUrtyGgEmdcy23qdyyE5///3OvjxPJ8zcFMItoPEsjWWMLA83Kf8/m+zgfyq77HK7/xL3/1IeYm4a07O08loaywiH7BhqUtds630a5wBrQxbs13eIuakJsYDGiyqk7+zvl14ox2KTyLuEQgo2lVKgBTRSVCEdd3mATf3SJQ9wWONGSLOFWkAjRRXSn32gbbtrHmi7gZDmCbaARrIfUvPPTF89Hm2oT2nr832X+oS8r3p5miF5XUAzxBaSz0X1mtJls714m0zgkMqjdpcQb8CFC8nzAprBfkgP5ftIlxBv0Z8CHj6AG+4AAAFlSURBVA/yvup9FfvkpzCHaQKFFHIBjRRPSMWZDdddgH2kS8jFlNKZDaFCCrqARoonpOpcuwA/wn2WvW1PeuR/KptQP7dbqJCCLqCRLI3F5ZBnmxDH7LK0QroWZ3/7nkop1M+MkIAkERIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgMB/6QDhdfNA9WQAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"2D random initialization\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_random <- matrix(rnorm(300), ncol=2)\n",
    "\n",
    "plot(y_random[,1], y_random[,2], pch=19, col=factor(iris_species), main=\"2D random initialization\", xlab=\"\", ylab=\"\", cex=0.5, cex.main=2)\n",
    "legend(\"topleft\", legend=levels(iris_species), pch=1, col=1:3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the cost function. As discussed in the lecture the cost function is given by:\n",
    "\n",
    "$\\Large CE(w_{high}(x_i, x_j), w_{low}(y_i, y_j)) = \\\\\n",
    "\\sum_i \\sum_j [ w_{high}(X) \\log(\\frac{w_{high}(X)}{w_{low}(Y)}) + (1- w_{high}(X)) \\log(\\frac{1-w_{high}(X)}{1-w_{low}(Y)})] $\n",
    "\n",
    "where $X$ stands for the distance between two points in high dimensional space and $Y$ stands for the distance in low dimensional space. Remember that later we will take the derivative of the cost function and therefore we can leave out the parts that only depend on $w_{high}(X)$ (because they are constant):\n",
    "\n",
    "$\\Large CE(w_{high}(x_i, x_j), w_{low}(y_i, y_j)) = \\\\\n",
    "\\sum_i \\sum_j [-w_{high}(X) \\log(w_{low}(Y)) + (1- w_{high}(X)) \\log(1-w_{low}(Y))]$\n",
    "\n",
    "\n",
    "The low dimensional weights can be obtained from the low dimensional distances $y$ from the function `low_dim_weight(y)`. Try to use matrix vectorization again instead of for-loops since it is a lot faster.\n",
    "\n",
    "For `y <- c(1:150)` the total cost function should be something like this: \n",
    "\n",
    "$\\text{cost function} = 4254$\n",
    "\n",
    "Take care however to use the averaged high dimensional edge weights (fuzzy union) when comparing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cost_function <- function(y, edge_weight_high){\n",
    "    #' @param y the low dimensional weights vector\n",
    "    #' @param edge_weight_high the high dimensional edge weights vector\n",
    "    #' @returns a cost function value, that represents how well the high dimensional structure \n",
    "    #'          is currently represented by the low dimensional structure. the lower the better.\n",
    "\n",
    "    # here goes your code\n",
    "\n",
    "    return(cost_function_total)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "4254.10854106655"
      ],
      "text/latex": [
       "4254.10854106655"
      ],
      "text/markdown": [
       "4254.10854106655"
      ],
      "text/plain": [
       "[1] 4254.109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_function <- function(y, edge_weight_high){\n",
    "    #' @param y the low dimensional weights vector\n",
    "    #' @param edge_weight_high the high dimensional edge weights vector\n",
    "    #' @returns a cost function value, that represents how well the high dimensional structure is currently\n",
    "    #' represented by the low dimensional structure. the lower the better.\n",
    "\n",
    "    edge_weight_low <- low_dim_weight(y)\n",
    "    cost_function_matrix <- -edge_weight_high * log(edge_weight_low + 0.01) - (1 - edge_weight_high) * log(1 - edge_weight_low + 0.01)\n",
    "\n",
    "    return(sum(cost_function_matrix))\n",
    "}\n",
    "\n",
    "y <- c(1:150)\n",
    "cost_function(y, high_dim_edge_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of cost function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need the derivative of the cost function. The equation of the cost function from the lecture is as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "\n",
    "\\large \\frac{\\partial CE(w_{low}(d_{ij}))}{\\partial y_i} &= \\text{ ... } = \\\\ \n",
    "\n",
    "\\Large &= \\sum_j \\Biggr[ \\frac{2abd_{ij}^{2(b-1)}w_{high}}{1 + ad_{ij}^{2b}} - \\frac{2b (1-w_{high})}{d_{ij}^2(1 + ad_{ij}^{2b})} \\Biggr] (y_i - y_j)\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "Below there are two implementation of the gradient of the cost function in R. The first one is fully vectorized and therefore harder to understand than the slightly less vectorized version below it. \n",
    "\n",
    "In both versions part of the prefactor (right_prefactor) was normalized. This is not part of the original UMAP algorithm, however including the normalization shows better performance on this manually programmed version of the UMAP algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 2 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td> 0.024556022</td><td>-0.064312169</td></tr>\n",
       "\t<tr><td>-0.002233739</td><td>-0.022882894</td></tr>\n",
       "\t<tr><td> 0.020781013</td><td>-0.065584653</td></tr>\n",
       "\t<tr><td>-0.033270805</td><td> 0.007680316</td></tr>\n",
       "\t<tr><td>-0.001407369</td><td>-0.017918261</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 2 of type dbl\n",
       "\\begin{tabular}{ll}\n",
       "\t  0.024556022 & -0.064312169\\\\\n",
       "\t -0.002233739 & -0.022882894\\\\\n",
       "\t  0.020781013 & -0.065584653\\\\\n",
       "\t -0.033270805 &  0.007680316\\\\\n",
       "\t -0.001407369 & -0.017918261\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 2 of type dbl\n",
       "\n",
       "|  0.024556022 | -0.064312169 |\n",
       "| -0.002233739 | -0.022882894 |\n",
       "|  0.020781013 | -0.065584653 |\n",
       "| -0.033270805 |  0.007680316 |\n",
       "| -0.001407369 | -0.017918261 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]         [,2]        \n",
       "[1,]  0.024556022 -0.064312169\n",
       "[2,] -0.002233739 -0.022882894\n",
       "[3,]  0.020781013 -0.065584653\n",
       "[4,] -0.033270805  0.007680316\n",
       "[5,] -0.001407369 -0.017918261"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#half vectorized version\n",
    "cost_gradient <- function(y, high_dim_edge_avg, a=1.93, b=0.79){\n",
    "    #' @param y the low dimensional weights vector\n",
    "    #' @param edge_weight_high_avg the \"average\" of the high dimensional edge weights vector\n",
    "    #' @returns the gradient of the cost function, i.e. in our case a matrix of shape 150x2\n",
    "    \n",
    "    n_ <- dim(y)[1] # n_ equals how many points/samples do we have in our data?\n",
    "    m_ <- dim(y)[2] # m_ equals the low dimensional space we project into? usually 2d or 3d.\n",
    "    y_i <- matrix(NaN, nrow=n_, ncol=m_)\n",
    "\n",
    "    d_ij <- as.matrix(dist(y, method = \"euclidean\", diag = TRUE, upper=TRUE))\n",
    "    left_prefactor <- a * high_dim_edge_avg * (c(1e-8) + d_ij**2)**(b-1)\n",
    "    right_prefactor <- (1 - high_dim_edge_avg) %*% (c(0.001) + d_ij**2)**(-1)\n",
    "    \n",
    "    diag(right_prefactor) <- 0\n",
    "    right_prefactor <- right_prefactor / rowSums(right_prefactor)\n",
    "    \n",
    "    prefactor <- 2 * b * low_dim_weight(y) * (left_prefactor - right_prefactor)\n",
    "\n",
    "    for (i in 1:n_){ # for loop over all points i\n",
    "\n",
    "        point_i <- y[i, ]\n",
    "        y_diff<- matrix(NaN, nrow=n_, ncol=m_)\n",
    "        for (j in 1:n_){ # for loop over all points j (in respect to point i)\n",
    "            point_j <- y[j, ]\n",
    "            y_diff[j, ] <- point_i - point_j\n",
    "        }\n",
    "        y_i[i, ] <- colSums(prefactor[i, ] * y_diff)\n",
    "\n",
    "    }\n",
    "    return(y_i)\n",
    "}\n",
    "\n",
    "y <- read.csv(\"../data/python_y_embedding_n_neigh=15.txt\", header=FALSE)\n",
    "y <- as.matrix(y)\n",
    "\n",
    "cost_gradient(y, high_dim_edge_avg)[1:5,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 2 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td> 0.024556022</td><td>-0.064312169</td></tr>\n",
       "\t<tr><td>-0.002233739</td><td>-0.022882894</td></tr>\n",
       "\t<tr><td> 0.020781013</td><td>-0.065584653</td></tr>\n",
       "\t<tr><td>-0.033270805</td><td> 0.007680316</td></tr>\n",
       "\t<tr><td>-0.001407369</td><td>-0.017918261</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 2 of type dbl\n",
       "\\begin{tabular}{ll}\n",
       "\t  0.024556022 & -0.064312169\\\\\n",
       "\t -0.002233739 & -0.022882894\\\\\n",
       "\t  0.020781013 & -0.065584653\\\\\n",
       "\t -0.033270805 &  0.007680316\\\\\n",
       "\t -0.001407369 & -0.017918261\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 2 of type dbl\n",
       "\n",
       "|  0.024556022 | -0.064312169 |\n",
       "| -0.002233739 | -0.022882894 |\n",
       "|  0.020781013 | -0.065584653 |\n",
       "| -0.033270805 |  0.007680316 |\n",
       "| -0.001407369 | -0.017918261 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]         [,2]        \n",
       "[1,]  0.024556022 -0.064312169\n",
       "[2,] -0.002233739 -0.022882894\n",
       "[3,]  0.020781013 -0.065584653\n",
       "[4,] -0.033270805  0.007680316\n",
       "[5,] -0.001407369 -0.017918261"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fully vectorized version\n",
    "cost_gradient <- function(y, high_dim_edge_avg, a=1.93, b=0.79){\n",
    "    #' @param y the low dimensional weights vector\n",
    "    #' @param edge_weight_high_avg the \"average\" of the high dimensional edge weights vector\n",
    "    #' @returns the gradient of the cost function, i.e. in our case a matrix of shape 150x2\n",
    "        \n",
    "    d_ij <- as.matrix(dist(y, method = \"euclidean\", diag = TRUE))\n",
    "    left_prefactor <- a * high_dim_edge_avg * (c(1e-8) + d_ij**2)**(b-1)\n",
    "    right_prefactor <- (1 - high_dim_edge_avg) %*% (c(0.001) + d_ij**2)**(-1)\n",
    "\n",
    "    diag(right_prefactor) <- 0\n",
    "    right_prefactor <- right_prefactor / rowSums(right_prefactor)\n",
    "\n",
    "    prefactor <- 2 * b * low_dim_weight(y) * (left_prefactor - right_prefactor)\n",
    "\n",
    "    dim(prefactor) <- c(dim(prefactor)[1], dim(prefactor)[-1], 1)\n",
    "\n",
    "    y_i <- array(rep(t(y), each=dim(y)[1]), c(dim(y)[1], dim(y)[-1], dim(y)[1]))\n",
    "    y_j <- array(y , c(dim(y)[1], dim(y)[-1], 1))\n",
    "    y_diff <- sweep(y_i, 1, y_j, \"-\", check.margin = FALSE)\n",
    "\n",
    "    result <- sweep(y_diff, 3, prefactor, \"*\", check.margin = FALSE)\n",
    "\n",
    "    return(t(colSums(result, dims=1)))\n",
    "}\n",
    "\n",
    "y <- read.csv(\"../data/python_y_embedding_n_neigh=15.txt\", header=FALSE)\n",
    "y <- as.matrix(y)\n",
    "\n",
    "cost_gradient(y, high_dim_edge_avg)[1:5,]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put everything together. We start with either a random initalization or the spectral embedding of the low dimensional representation of our data. Then we calculate the gradient of the cost function and take a step in the correct direction in the low dimensional space, i.e.\n",
    "\n",
    "$\\Large y_i = y_i - \\alpha \\cdot \\frac{\\partial CE}{\\partial y_i}$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\frac{\\partial CE}{\\partial y_i}$ is the i-th element of the gradient of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function = 0.062772295319594 after 10 iterations.\n",
      "Cost function = 0.0624597873962243 after 20 iterations.\n",
      "Cost function = 0.0545135472465257 after 30 iterations.\n",
      "Cost function = 0.0589153098399367 after 40 iterations.\n",
      "Cost function = 0.0552275245016194 after 50 iterations.\n",
      "Cost function = 0.0537187868803247 after 60 iterations.\n",
      "Cost function = 0.0484683203864469 after 70 iterations.\n",
      "Cost function = 0.0601554906756562 after 80 iterations.\n",
      "Cost function = 0.0474315915004974 after 90 iterations.\n",
      "Cost function = 0.0502327069378156 after 100 iterations.\n",
      "Cost function = 0.0506839140941198 after 110 iterations.\n",
      "Cost function = 0.0573471445237112 after 120 iterations.\n",
      "Cost function = 0.0503205381661872 after 130 iterations.\n",
      "Cost function = 0.0547711917603866 after 140 iterations.\n",
      "Cost function = 0.0480662952391175 after 150 iterations.\n",
      "Cost function = 0.0465456530874574 after 160 iterations.\n",
      "Cost function = 0.0498129204420093 after 170 iterations.\n",
      "Cost function = 0.0470649485702953 after 180 iterations.\n",
      "Cost function = 0.044586321262245 after 190 iterations.\n",
      "Cost function = 0.0507008321340861 after 200 iterations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>null device:</strong> 1"
      ],
      "text/latex": [
       "\\textbf{null device:} 1"
      ],
      "text/markdown": [
       "**null device:** 1"
      ],
      "text/plain": [
       "null device \n",
       "          1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title \"Convergence of the cost function\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_low_dim <- 2\n",
    "learning_rate_alpha <- 1\n",
    "max_iter <- 200\n",
    "\n",
    "## starting point for the low dimensional space through spectral embedding\n",
    "# y <- read.csv(\"../data/python_y_embedding_n_neigh=15.txt\", header=FALSE)\n",
    "# y <- as.matrix(y)\n",
    "\n",
    "## random initialization\n",
    "y_random <- matrix(rnorm(300), ncol=2)\n",
    "y <- y_random\n",
    "\n",
    "cost_function_vec <- c()\n",
    "\n",
    "for (iteration_i in 1:max_iter){\n",
    "    \n",
    "    y <- y - learning_rate_alpha * cost_gradient(y, high_dim_edge_avg, a=1.93, b=0.79)\n",
    "\n",
    "    # plotting\n",
    "    dir.create(file.path(\"./plots/UMAP_iris_r\"), showWarnings = FALSE)\n",
    "    png(paste0(\"./plots/UMAP_iris_r/iris_iter_\",iteration_i,\".png\"), width=500, height=500)\n",
    "    plot(y[,1], y[,2], pch=19, col=as.numeric(iris_species),\n",
    "        main=paste(\"UMAP on Iris data set\\niteration = \", iteration_i), xlab=\"UMAP1\", ylab=\"UMAP2\", cex=0.5, cex.main=2)\n",
    "    legend(\"bottomleft\", legend=levels(iris_species), pch=1, col=1:3)\n",
    "    dev.off()\n",
    "\n",
    "    # cost function value\n",
    "    cost_function_current <- sum(cost_function(y, high_dim_edge_avg)) / 1e+5\n",
    "    cost_function_vec <- c(cost_function_vec, cost_function_current)\n",
    "\n",
    "    # algorithm progress\n",
    "    if (iteration_i %% 10 == 0){\n",
    "        cat(paste0(\"Cost function = \", cost_function_current, \" after \", iteration_i, \" iterations.\\n\"))\n",
    "    }\n",
    "}\n",
    "\n",
    "plot(cost_function_vec, main=\"Convergence of the cost function\", xlab=\"iterations\", \n",
    "    ylab=\"cost function value\", cex.main=2, cex.lab=1.5)\n",
    "dev.off()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different iterations are saved in the 'src/plots/UMAP_iris_r' directory. By clicking through the images you can see how the algorithm slowly converges.\n",
    "\n",
    "After conversion is completed our manually programmed UMAP algorithm shows the following low dimensional representation of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title \"UMAP on Iris data set\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(y[,1], y[,2], pch=19, col=factor(iris_species), main=\"UMAP on Iris data set\", \n",
    "    xlab=\"UMAP1\", ylab=\"UMAP2\", cex=0.5, cex.main=2)\n",
    "legend(\"topright\", legend=levels(iris_species), pch=1, col=1:3)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already seen in the lecture the manually programmed UMAP algorithm has a hard time distinguishing between the classes 'versicolor' and 'virginica'. You can try and play around with the hyperparamters to achieve a better result for yourself. Try increasing the number of $k$ neighbors to better retain the global structure of the high dimensional space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are finished!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
